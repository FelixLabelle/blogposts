{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.backend import clear_session\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import io\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "from keras.backend import clear_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.tools as tools\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.graph_objs as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "import os\n",
    "import pickle\n",
    "import unicodedata\n",
    "from tqdm import tqdm_notebook, tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Model and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layers = 2\n",
    "dropout = 0.7\n",
    "n_hidden = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "epochs = 50\n",
    "starting_epoch = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tools.set_credentials_file(username='USERNAME', api_key='APIKEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Experiment path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "experiment_name = \".\".join([str(n_hidden) + \".\" + str(dropout)]*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = os.path.join(r'training_files',experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_movies = pd.read_pickle(\"df_movies_cluster.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "christmas_transcripts = df_movies[df_movies.cluster_name==\"christmas\"].subtitles_text.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_text = ' '.join([x.lower() for x in christmas_transcripts])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chars = sorted(list(set(all_text)))\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 40\n",
    "step = 5\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(all_text) - maxlen, step):\n",
    "    sentences.append(all_text[i: i + maxlen])\n",
    "    next_chars.append(all_text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"char_indices.pickle\",'rb') as file:\n",
    "    char_indices = pickle.load(file)\n",
    "    \n",
    "with open(\"indices_char.pickle\",'rb') as file:\n",
    "    indices_char = pickle.load( file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in tqdm(enumerate(sentences)):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x,y,random_state=11, test_size =0.2, train_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "for i in range(layers-1):\n",
    "    model.add(LSTM(n_hidden, input_shape=(maxlen, len(chars)),recurrent_dropout=dropout , return_sequences=True))\n",
    "model.add(LSTM(n_hidden,input_shape=(maxlen, len(chars)),recurrent_dropout=dropout))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1. JSON logging callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def epoch_end(epoch, logs):\n",
    "    with open(os.path.join(path,'loss.logg.json'), mode='a') as file:\n",
    "        file.write(json.dumps({\n",
    "            \"epoch\": epoch,\n",
    "            \"loss\": logs['loss'],\n",
    "            \"val_loss\": logs[\"val_loss\"],\n",
    "            \"time\": time.time()\n",
    "        }) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "json_logging_callback = LambdaCallback(\n",
    "    on_epoch_end=epoch_end \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2. Model weights callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filepath=os.path.join(path,\"periodic_weights.{epoch:02d}-{val_loss:.2f}.hdf5\")\n",
    "checkpoint_5 = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, mode='min', period=5)\n",
    "\n",
    "filepath=os.path.join(path,\"best_weights.{epoch:02d}-{val_loss:.2f}.hdf5\")\n",
    "checkpoint_best = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=epochs,\n",
    "          initial_epoch = 27,\n",
    "          verbose=False,\n",
    "          validation_data=(x_test,y_test),\n",
    "          callbacks=[json_logging_callback,checkpoint_best, checkpoint_5, TQDMNotebookCallback(leave_inner=True,leave_outer=True)]\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4. Analyze logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_unified_loss(path):\n",
    "    records = []    \n",
    "    for f in os.listdir(path):\n",
    "        if f.endswith(\".json\"):\n",
    "            with open(os.path.join(path,f),\"r\") as file:\n",
    "                for l in file.readlines():\n",
    "                    records.append(json.loads(l.strip('\\n')))\n",
    "            \n",
    "    return sorted(records, key=lambda x: x[\"epoch\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_traces(analysis, experiments):\n",
    "    traces = []\n",
    "    for key, exp in experiments.items():\n",
    "        path = os.path.join('training_files',exp)\n",
    "        loss = get_unified_loss(path)\n",
    "        \n",
    "        times = [x[\"time\"] for x in loss]\n",
    "        diffs = [times[i+1] - time for i, time in enumerate(times[:-1])]\n",
    "        print(np.mean(diffs))\n",
    "\n",
    "        traces.append(go.Scatter(\n",
    "            x=list(range(len(loss))),\n",
    "            y=[x[\"loss\"] for x in loss],\n",
    "            name=key + \" training\"\n",
    "        ))\n",
    "\n",
    "        traces.append(go.Scatter(\n",
    "            x=list(range(len(loss))),\n",
    "            y=[x[\"val_loss\"] for x in loss],\n",
    "            name=key + \" test\"\n",
    "        ))\n",
    "\n",
    "\n",
    "\n",
    "    layout = go.Layout(\n",
    "        title='Effect of changing '+ analysis,\n",
    "        xaxis=dict(\n",
    "            title='Epoch',\n",
    "            titlefont=dict(\n",
    "                size=18,\n",
    "                color='#7f7f7f'\n",
    "            )\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title='Loss',\n",
    "            titlefont=dict(\n",
    "                size=18,\n",
    "                color='#7f7f7f'\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return traces, layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Dropout rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "analysis = \"dropout\"\n",
    "experiments = {\n",
    "    \"0 dropout\":'512.0.0.512.0.0', \n",
    "    \"0.35 dropout\": '512.0.35.512.0.35',\n",
    "    \"0.7 dropout\": '512.0.7.512.0.7'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "traces, layout = get_traces(analysis, experiments)\n",
    "fig = go.Figure(data=traces, layout=layout)\n",
    "py.iplot(fig, filename='td_medium_nlp_'+  analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. N° of Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "analysis = \"layers\"\n",
    "experiments = {\n",
    "    \"1 layer\":'512.0.7', \n",
    "    \"2 layers\": '512.0.7.512.0.7',\n",
    "    \"3 layers\": '512.0.7.512.0.7.512.0.7'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "traces, layout = get_traces(analysis, experiments)\n",
    "fig = go.Figure(data=traces, layout=layout)\n",
    "py.iplot(fig, filename='td_medium_nlp_'+  analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Hidden units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "analysis = \"hidden units\"\n",
    "experiments = {\n",
    "    \"128 units\":'128.0.7.128.0.7', \n",
    "    \"256 units\": '256.0.7.256.0.7',\n",
    "    \"512 units\": '512.0.7.512.0.7'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "traces, layout = get_traces(analysis, experiments)\n",
    "fig = go.Figure(data=traces, layout=layout)\n",
    "py.iplot(fig, filename='td_medium_nlp_'+  analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Full training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "analysis = \"data\"\n",
    "experiments = {\n",
    "    \"20% training data\":'512.0.7.512.0.7', \n",
    "    \"80% training data\": '512.0.7.512.0.7.full'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "traces, layout = get_traces(analysis, experiments)\n",
    "fig = go.Figure(data=traces, layout=layout)\n",
    "py.iplot(fig, filename='td_medium_nlp_'+  analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5. Text generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Load best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "experiment_name = \"512.0.7.512.0.7\"\n",
    "path = os.path.join(r'training_files',experiment_name, 'best_weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.load_weights(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Generate with diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "diversity = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_seed = \"are you okay, didn't you hear the jingle bells\"\n",
    "start_seed = start_seed[:40]\n",
    "print(start_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentence = start_seed\n",
    "generated = ''\n",
    "generated += start_seed\n",
    "\n",
    "for i in tqdm_notebook(range(1000)):\n",
    "    x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "    for t, char in enumerate(sentence):\n",
    "        x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "    preds = model.predict(x_pred, verbose=0)[0]\n",
    "    next_index = sample(preds, diversity)\n",
    "    next_char = indices_char[next_index]\n",
    "\n",
    "    generated += next_char\n",
    "    sentence = sentence[1:] + next_char\n",
    "\n",
    "print(generated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

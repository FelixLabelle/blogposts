{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Imports and config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.tools as tools\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.graph_objs as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from http.client import ResponseNotReady, CannotSendRequest\n",
    "from xmlrpc.client import ProtocolError\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import dateparser\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import re\n",
    "import imdb\n",
    "from tqdm import tqdm\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "import time\n",
    "from pythonopensubtitles.opensubtitles import OpenSubtitles\n",
    "import gzip\n",
    "import shutil\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "import pickle\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bokeh.palettes import Spectral11\n",
    "from bokeh.plotting import figure, output_file, show\n",
    "from bokeh.models import ColumnDataSource, Range1d, LabelSet, Label, HoverTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim import corpora\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM, Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.backend import clear_session\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras_tqdm import TQDMNotebookCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2. Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tools.set_credentials_file(username='USERNAME', api_key='APIKEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Title Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/List_of_Hallmark_Channel_Original_Movies\"\n",
    "r  = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = r.text\n",
    "soup = BeautifulSoup(data, \"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soup analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2000 - 2009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "years_00_09 = [x.text for x in soup.find_all(\"span\", {\"class\": \"mw-headline\", \"id\" : lambda L: L and L.startswith('200')}) if not \"–\" in x.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ols = [x for x in soup.find_all(\"ol\")][:len(years_00_09)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "movies_00_09 = []\n",
    "\n",
    "for i, movie in enumerate(ols):\n",
    "    lists = movie.find_all(\"li\")\n",
    "    for li in lists:\n",
    "        name = li.find(\"i\").text\n",
    "        date = li.find_all(text=True, recursive=False)\n",
    "        date = [x.text for x in nlp(str(date[0])).ents if x.label_ == \"DATE\"]\n",
    "        \n",
    "        final_date = 2000\n",
    "        \n",
    "        if len(date) == 0:\n",
    "            final_date = years_00_09[i]\n",
    "        else:\n",
    "            final_date = date[0]\n",
    "            \n",
    "            # some of the earlier ones miss their year\n",
    "            if not years_00_09[i] in final_date:\n",
    "                final_date = final_date + \", \" + years_00_09[i]\n",
    "                \n",
    "        \n",
    "        # format the date\n",
    "        date_formatted = dateparser.parse(final_date)\n",
    "        \n",
    "        if not date_formatted:\n",
    "            print(final_date)\n",
    "            \n",
    "        movies_00_09.append((name ,date_formatted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2010 - 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "years = [x.text for x in soup.find_all(\"span\", {\"class\": \"mw-headline\", \"id\" : lambda L: L and L.startswith('201')}) if not \"–\" in x.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "movies_10_18 = []\n",
    "tables = soup.find_all('table', {'class':'wikitable sortable'})\n",
    "\n",
    "index_of_interest = [3,5,7,9,11,13,15,17,19]\n",
    "\n",
    "tables_of_interest = [tables[i] for i in index_of_interest]\n",
    "\n",
    "for table in tqdm_notebook(tables_of_interest):\n",
    "    headers = [x.text for x in table.find_all('th')]\n",
    "    \n",
    "    rows = table.find_all(\"tr\")\n",
    "    for row in rows:\n",
    "        cols = row.find_all('td')\n",
    "        if len(cols) > 0:\n",
    "            \n",
    "            cols = [cols[i] for i in range(len(cols)) if headers[i].strip(\"\\n\") == \"Movie\" or headers[i].strip(\"\\n\") == \"Original airdate\"]\n",
    "            cols = [ele.text.strip() for ele in cols]\n",
    "            \n",
    "            movies_10_18.append((cols[0], dateparser.parse(cols[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One big happy dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_movies = pd.DataFrame({\n",
    "    \"title\":[x[0] for x in movies_00_09 + movies_10_18 ],\n",
    "    \"date\":[x[1] for x in movies_00_09 + movies_10_18 ]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_movies.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How different are they"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keywords = [\"royal\",\"christmas\",\"wedding\",\"prince\", \"love\"]\n",
    "permutations = list(itertools.permutations(keywords, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_lemmas(title):\n",
    "    doc = nlp(title)\n",
    "    return [x.lemma_.lower() for x in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_subset(a,b):\n",
    "    return any(set(x).issubset(b) for x in a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_movies[\"title_lemma\"] = df_movies[\"title\"].apply(get_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_movies[\"a_in_b\"] = df_movies.apply(lambda x: is_subset(permutations, x['title_lemma']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_movies[df_movies[\"a_in_b\"] == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other movie name ideas:\n",
    "- a christmas prince\n",
    "- a prince's wedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many movies per month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_movies_date = df_movies.copy()\n",
    "df_movies_date.set_index(\"date\", inplace=True)\n",
    "df_movies_date.index = pd.DatetimeIndex(df_movies_date.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_movies_date_count_christmas = df_movies_date[df_movies_date.title.str.contains(\"Christmas\")].resample('M').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_movies_date_count = df_movies_date.resample('M').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trace1 = go.Scatter(\n",
    "    x=df_movies_date_count[df_movies_date_count.index.year > 2009].index,\n",
    "    y=df_movies_date_count[df_movies_date_count.index.year > 2009].title,\n",
    "    fill='tozeroy',\n",
    "    mode=\"none\",\n",
    "    name=\"... in general\"\n",
    ")\n",
    "\n",
    "trace2 = go.Scatter(\n",
    "    x=df_movies_date_count_christmas[df_movies_date_count_christmas.index.year > 2009].index,\n",
    "    y=df_movies_date_count_christmas[df_movies_date_count_christmas.index.year > 2009].title,\n",
    "    fill='tozeroy',\n",
    "    mode=\"none\",\n",
    "    name=\"... with 'Christmas' in title\"\n",
    ")\n",
    "\n",
    "\n",
    "layout = go.Layout(\n",
    "    title='Movies launched per month',\n",
    "    xaxis=dict(\n",
    "        title='Month',\n",
    "        titlefont=dict(\n",
    "            size=18,\n",
    "            color='#7f7f7f'\n",
    "        )\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='Title launches',\n",
    "        titlefont=dict(\n",
    "            size=18,\n",
    "            color='#7f7f7f'\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "data = [trace1,trace2]\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='td_medium_nlp_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most popular words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stop_en = stopwords.words(\"English\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lemmas =[x for sublist in df_movies.title_lemma for x in sublist if not \n",
    "         (x in stop_en \n",
    "          or x in list(punctuation)\n",
    "          or x in [\"-pron-\"]\n",
    "         )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for x in Counter(lemmas).most_common(20):\n",
    "    print(x[0],\",\",x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_movies.to_pickle(\"df_movies_base.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_movies = pd.read_pickle(\"df_movies_base.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Subtitles scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get IMDB id for every movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ia = imdb.IMDb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_imdb_id(title, year):\n",
    "\n",
    "    movieid = 0\n",
    "    movieyear = None\n",
    "    movies = ia.search_movie(title)\n",
    "    if movies:\n",
    "        if len(movies) == 1:\n",
    "            if \"year\" in movies[0].keys():\n",
    "                movieyear = movies[0][\"year\"]\n",
    "\n",
    "            if \"title\" in movies[0].keys():\n",
    "                movietitle = movies[0][\"title\"]\n",
    "\n",
    "            movieid = movies[0].movieID\n",
    "            \n",
    "        elif len(movies) > 1:      \n",
    "            for mov in movies:\n",
    "                if 'title' in mov.keys() and mov[\"title\"].lower() == title.lower():\n",
    "                    if \"year\" in mov.keys():\n",
    "                        movieyear = mov[\"year\"]\n",
    "                        \n",
    "                    movieid = mov.movieID\n",
    "                    break\n",
    "\n",
    "            if movieyear:\n",
    "                if year in [movieyear - 1, movieyear, movieyear + 1]:\n",
    "                    return movieid\n",
    "                else:\n",
    "                    return None\n",
    "            else:\n",
    "                return movieid\n",
    "        \n",
    "            \n",
    "    else:\n",
    "        title_nosquare = re.sub(r'\\[[^)]*\\]', '', title).strip()\n",
    "        title_nonothing = re.sub(r'\\([^)]*\\)', '', title_nosquare).strip()\n",
    "        \n",
    "        # check if squares are a match\n",
    "        if title == title_nosquare:\n",
    "            pass\n",
    "        else:\n",
    "            movieid = get_imdb_id(title_nosquare, year)\n",
    "           \n",
    "        # check if nothing is a match\n",
    "        if not movieid:\n",
    "            if title == title_nonothing:\n",
    "                return None\n",
    "            else:\n",
    "                movieid = get_imdb_id(title_nonothing, year)\n",
    "        \n",
    "    return movieid\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "print(get_imdb_id(\"A Royal Winter (Winterfest)[229]\",2017))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tqdm_notebook.pandas(desc=\"my bar!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df_movies[\"imdb_id\"] = df_movies.progress_apply(lambda x: get_imdb_id(x[\"title\"], x['date'].year), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_movies.to_pickle(\"df_movies_imdbid.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_movies = pd.read_pickle(\"df_movies_imdbid.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get download links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ost = OpenSubtitles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token = ost.login('EMAIL', 'PASSWORD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert type(token) == str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_download_link(imdb_id):\n",
    "    \n",
    "    link = None\n",
    "    \n",
    "    if imdb_id:\n",
    "        data = ost.search_subtitles([{'sublanguageid': 'en', 'imdbid':str(imdb_id)}])\n",
    "        \n",
    "        if data and len(data) > 0:\n",
    "            for y in data:\n",
    "                if y[\"ISO639\"] == 'en':\n",
    "                    print(\"found english sub for movie \", str(imdb_id))\n",
    "                    link = y[\"SubDownloadLink\"]\n",
    "    \n",
    "    time.sleep(10)\n",
    "    return link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subtitle_links = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i_no_new_found = 0\n",
    "\n",
    "while i_no_new_found < 1:\n",
    "    \n",
    "    xFound = False\n",
    "    \n",
    "    for i, row in tqdm_notebook(df_movies[(~df_movies[\"imdb_id\"].isin(subtitle_links.keys())) & (~pd.isnull(df_movies[\"imdb_id\"]))].iterrows()):\n",
    "        try:\n",
    "            result = get_download_link(row[\"imdb_id\"])\n",
    "            if result:\n",
    "                subtitle_links[row[\"imdb_id\"]] = result\n",
    "                xFound = True\n",
    "                i_no_new_found = 0\n",
    "        except ResponseNotReady as e:\n",
    "            print(\"response-not-ready-error\", time.ctime())\n",
    "            time.sleep(5)\n",
    "            pass\n",
    "        except CannotSendRequest as e:\n",
    "            print(\"cannot-send-request-error\", time.ctime())\n",
    "            time.sleep(5)\n",
    "        except ProtocolError as e:\n",
    "            print(\"protocol-error\", time.ctime())\n",
    "            time.sleep(5)\n",
    "            pass\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "            \n",
    "    if not xFound:\n",
    "        i_no_new_found += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"ost_subtitle_links_dict.pickle\",'wb') as file:\n",
    "    pickle.dump(subtitle_links,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"ost_subtitle_links_dict.pickle\",'rb') as file:\n",
    "    subtitle_links = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_movies[\"downloadlink\"] = df_movies[\"imdb_id\"].map(subtitle_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_movies.to_pickle(\"df_movies_links.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_movies = pd.read_pickle(\"df_movies_links.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Download subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_movies_sorted = df_movies[~pd.isnull(df_movies['downloadlink'])].sort_values(by=\"imdb_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download_unzip_subtitle(link, name):\n",
    "    if not pd.isnull(link):\n",
    "        try:\n",
    "            time.sleep(1)\n",
    "            gz_name = name + '.gz'\n",
    "            with open(gz_name, \"wb\") as f:\n",
    "                r = requests.get(link)\n",
    "                f.write(r.content)\n",
    "\n",
    "\n",
    "            with gzip.open(gz_name, 'rb') as f_in:\n",
    "                with open(os.path.join(\"downloaded_subtitles\", name + '.txt'), 'wb') as f_out:\n",
    "                    shutil.copyfileobj(f_in, f_out)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(name)\n",
    "            print(link)\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for i, row in tqdm_notebook(df_movies_sorted.iterrows()):\n",
    "    if not row[\"imdb_id\"] in [x.strip(\".txt\") for x in os.listdir(\"downloaded_subtitles\")]:\n",
    "        download_unzip_subtitle(row[\"downloadlink\"], row[\"imdb_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Process the data\n",
    "We need to read in all of the downloaded subs and process them a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "basepath = \"downloaded_subtitles/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "re_string = re.compile(\"\\[{1}\\S+\\]{1}:{1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_subtitle_text(txt):\n",
    "    cleanlines = []\n",
    "    \n",
    "    for line in txt:\n",
    "        line = line.strip(\"\\n\")\n",
    "        line = line.replace(\"\\'\",\"'\")\n",
    "        line = line.strip(\" - \")\n",
    "        line = line.replace(\"<i>\",'').replace(\"</i>\",\"\")\n",
    "        if line.isupper():\n",
    "            line = line.lower()\n",
    "    \n",
    "        # For each line: remove if it is just a number:\n",
    "        if line.isdigit():\n",
    "            continue\n",
    "          \n",
    "        # Also: if it is a time range: remove it\n",
    "        elif len(line.split(\" --> \")) == 2:\n",
    "            continue\n",
    "            \n",
    "        # If not: it is a valid spoken line, but some cleaning is still needed\n",
    "        else:\n",
    "            # Excess points indicate a run-on, so remove an uppercase\n",
    "            if line[:3] == '...':\n",
    "                line = line[4:]\n",
    "                line = line[0].lower() + line[1:]\n",
    "                \n",
    "                \n",
    "            if line[-3:] == '...':\n",
    "                line = line.strip(\"...\")\n",
    "            \n",
    "            # Remove names from in between brackets, unless it is actually a usefull words. Example: [Nikki]:\n",
    "            for ref in re_string.findall(line):\n",
    "                line = line.replace(ref,\"\")\n",
    "            \n",
    "            cleanlines.append(line)\n",
    "            \n",
    "    return ' '.join(cleanlines).replace(\"  \",\" \").replace(\"  \",\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sub_dict = {}\n",
    "for sub in tqdm_notebook(os.listdir(basepath)):\n",
    "    if sub[-4:] == \".txt\":\n",
    "        with open(os.path.join(basepath,sub),\"r\",encoding='utf8', errors=\"replace\") as file:\n",
    "            try:\n",
    "                file_text = file.readlines()\n",
    "                sub_dict[sub[:-4]] = process_subtitle_text(file_text)\n",
    "            except KeyboardInterrupt:\n",
    "                raise\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_movies[\"subtitles_text\"] = df_movies.imdb_id.map(sub_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_movies.to_pickle(\"df_movies_subtitles.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_movies = pd.read_pickle(\"df_movies_subtitles.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "problems:\n",
    "- camelcase\n",
    "- non-eng\n",
    "- sometimes still 'NICK: Hey! How's it going' > remove name, 'JENNIE: I\\'m sure. B'\n",
    "- still lots of \\x00\n",
    "- sometimes some </ i> tags\n",
    "- \\ufeff1\n",
    "- <font color=\"#9ae965\">sync and correction by solfieri www.addic7ed.com</font>'\n",
    "- remove everything between brackets? not sure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea is that a movie probably consists of a number of diverse topics, I would definitely expect things like:\n",
    "- love\n",
    "- weddings\n",
    "- christmas\n",
    "- food?\n",
    "\n",
    "To be distinct topics.\n",
    "\n",
    "So let's see if LDA agrees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Document processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stopwords_en = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subtitles_text = df_movies[~pd.isnull(df_movies.subtitles_text)][\"subtitles_text\"].values\n",
    "subtitles_imdbid = df_movies[~pd.isnull(df_movies.subtitles_text)][\"imdb_id\"].values\n",
    "subtitles_titles = df_movies[~pd.isnull(df_movies.subtitles_text)][\"title\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "processed_docs = [[x.lemma_ for x in nlp(y) if not x.ent_type_ == 'PERSON'] for y in tqdm_notebook(subtitles_text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"processed_docs_spacy.pickle\",\"wb\") as file:\n",
    "    pickle.dump(processed_docs, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"processed_docs_spacy.pickle\",\"rb\") as file:\n",
    "    processed_docs = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "processed_docs_filtered = [[x for x in y  if not x in stopwords_en and not len(x) < 2 ] for y in tqdm_notebook(processed_docs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Gensim LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_topics = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(processed_docs)\n",
    "dictionary.filter_extremes(no_below=30, no_above=0.5, keep_n=1500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check if we didn't cut out too much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"christmas\" in dictionary.token2id)\n",
    "print(\"santa\" in dictionary.token2id)\n",
    "print(\"wed\" in dictionary.token2id)\n",
    "print(\"ho\" in dictionary.token2id)\n",
    "print(\"bride\" in dictionary.token2id)\n",
    "print(\"cake\" in dictionary.token2id)\n",
    "print(\"groom\" in dictionary.token2id)\n",
    "print(\"princess\" in dictionary.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Launch algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda_model = LdaMulticore(bow_corpus, id2word=dictionary, passes=2, workers=2, num_topics=num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. SKlearn TFIDF + LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def identity_tokenizer(text):\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_df=0.5, min_df=30, tokenizer=identity_tokenizer,  lowercase=False)\n",
    "processed_docs_tfidf = tfidf.fit_transform(processed_docs_filtered)\n",
    "tf_feature_names = tfidf.get_feature_names()\n",
    "processed_docs_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check if we didn't cut out too much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"christmas\" in tf_feature_names)\n",
    "print(\"santa\" in tf_feature_names)\n",
    "print(\"wed\" in tf_feature_names)\n",
    "print(\"ho\" in tf_feature_names)\n",
    "print(\"bride\" in tf_feature_names)\n",
    "print(\"cake\" in tf_feature_names)\n",
    "print(\"groom\" in tf_feature_names)\n",
    "print(\"princess\" in tf_feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Launch algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "lda = LatentDirichletAllocation(n_components=num_topics, max_iter=10, learning_method='online', learning_offset=50.,random_state=0).fit(processed_docs_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print (\"Topic %d:\" % (topic_idx))\n",
    "        print (\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "no_top_words = 10\n",
    "display_topics(lda, tf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so LDA didn't exactly fetch out the topics I was hoping for. I did a lot of playing around afterwards as well, trying different hyperparameters, preprocessing methods and topic modelling algo's, but none were really satisfactory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic idea: since topic modelling didn't really allow us to select a subset of movies, perhaps document clustering will. We will represent each document as a TFIDF vector, and perform K-Means clustering on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_clusters = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=num_clusters, max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "km.fit(processed_docs_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mapping = dict(zip(subtitles_imdbid,clusters ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_movies[\"cluster\"] = df_movies.imdb_id.map(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Visualize clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T-SNE dimens. reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=1000)\n",
    "tsne_results = tsne.fit_transform(processed_docs_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Color dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate color dict\n",
    "\n",
    "spectral = Spectral11\n",
    "colors = []\n",
    "\n",
    "for cluster in clusters:\n",
    "    colors.append(spectral[cluster])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "traces = []\n",
    "\n",
    "data = list(zip(tsne_results,df_movies[~pd.isnull(df_movies.cluster)].title.tolist(), df_movies[~pd.isnull(df_movies.cluster)].cluster.tolist(), colors ))\n",
    "\n",
    "for i in range(num_clusters):\n",
    "    \n",
    "    point_slice = np.asarray([x[0] for x in data if x[2] == i])\n",
    "    title_slice = [x[1] for x in data if x[2] == i]\n",
    "    \n",
    "    trace = go.Scatter(\n",
    "        x=point_slice[:, 0],\n",
    "        y=point_slice[:, 1],\n",
    "        text = title_slice,\n",
    "        hoverinfo = 'text',\n",
    "        marker=dict(\n",
    "            size=16,\n",
    "            color = spectral[i], #set color equal to a variable\n",
    "            showscale=False\n",
    "        ),\n",
    "        mode=\"markers\",\n",
    "        name=\"cluster {}\".format(i)\n",
    "    )\n",
    "    \n",
    "    traces.append(trace)\n",
    "\n",
    "\n",
    "layout = go.Layout(\n",
    "    title='Clustering of Hallmark movies',\n",
    "    xaxis=dict(\n",
    "        title='X tsne value',\n",
    "        titlefont=dict(\n",
    "            size=18,\n",
    "            color='#7f7f7f'\n",
    "        )\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='Y tsne value',\n",
    "        titlefont=dict(\n",
    "            size=18,\n",
    "            color='#7f7f7f'\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=traces, layout=layout)\n",
    "py.iplot(fig, filename='td_medium_nlp_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cluster naming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cluster_dict = {\n",
    "    0:\"wedding\",\n",
    "    1:\"lovestory_1\",\n",
    "    2:\"fantasy\",\n",
    "    3:\"seasonal_love\",\n",
    "    4:\"christmas\",\n",
    "    5:\"lovestory_2\",\n",
    "    6:\"food\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_movies[\"cluster_name\"] = df_movies.cluster.map(cluster_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_movies.to_pickle(\"df_movies_cluster.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_movies = pd.read_pickle(\"df_movies_cluster.pickle\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "widgets": {
   "state": {
    "93d6bcf327ac4e83afb72478d8c483d4": {
     "views": [
      {
       "cell_index": 25
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
